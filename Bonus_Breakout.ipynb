{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imresize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: BreakoutDeterministic-v4\n",
      "[2017-11-29 02:38:50,060] Making new env: BreakoutDeterministic-v4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# print(gym.envs.registry.all())\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "observation = env.reset() # This gets us the image\n",
    "print (observation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_close',\n",
       " '_closed',\n",
       " '_elapsed_seconds',\n",
       " '_elapsed_steps',\n",
       " '_ensure_no_double_wrap',\n",
       " '_env_closer_id',\n",
       " '_episode_started_at',\n",
       " '_max_episode_seconds',\n",
       " '_max_episode_steps',\n",
       " '_owns_render',\n",
       " '_past_limit',\n",
       " '_render',\n",
       " '_reset',\n",
       " '_seed',\n",
       " '_spec',\n",
       " '_step',\n",
       " 'action_space',\n",
       " 'class_name',\n",
       " 'close',\n",
       " 'configure',\n",
       " 'env',\n",
       " 'metadata',\n",
       " 'observation_space',\n",
       " 'render',\n",
       " 'reset',\n",
       " 'reward_range',\n",
       " 'seed',\n",
       " 'spec',\n",
       " 'step',\n",
       " 'unwrapped']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame, reward, is_done, _ = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3) uint8\n"
     ]
    }
   ],
   "source": [
    "print (frame.shape, frame.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4b76492e10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nITJL+j+3QFvlMGmcOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b78a235f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_grayscale(img):\n",
    "    return np.dot(img[...,:3], [0.299, 0.587, 0.114]).astype(np.uint8)\n",
    "\n",
    "def crop(img,cropx=None,cropy=None):\n",
    "    y,x,c = img.shape\n",
    "    if (cropx is None) :\n",
    "        cropx = min(y, x)\n",
    "    if (cropy is None) :\n",
    "        cropy = min(y, x)\n",
    "    startx = x//2 - cropx//2\n",
    "    starty = y - cropy    \n",
    "    return img[starty:starty+cropy, startx:startx+cropx, :]\n",
    "\n",
    "def preprocess(img):\n",
    "#     return imresize(to_grayscale(crop(img))/127.5-1, (84, 84), 'cubic', 'F')\n",
    "    return imresize(crop(img)/127.5-1, (84, 84), 'cubic', 'RGB')\n",
    "\n",
    "def transform_reward(reward):\n",
    "        return np.sign(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 3) uint8 255 0 46.4648053666\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfLMMOfhmGfp/zI4G5qUmq78AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b764079b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = preprocess(frame)\n",
    "plt.imshow(a)\n",
    "print (a.shape, a.dtype, np.max(a), np.min(a), np.mean(a))\n",
    "# plt.imshow(to_grayscale(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[21168*4]) # 84 x 84 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addToHistory(his, s):\n",
    "    his.pop(0)\n",
    "    his.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def his2np(his):\n",
    "    return np.concatenate(tuple(his), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def state2input(his, s):\n",
    "    addToHistory(his, preprocess(s))\n",
    "    return his2np(his)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "(84672,)\n",
      "(84672,)\n"
     ]
    }
   ],
   "source": [
    "state_history = []\n",
    "s = env.reset()\n",
    "for idx in range(4):\n",
    "    state_history.append(preprocess(s))\n",
    "print (s.shape)\n",
    "s = state2input(state_history, s)\n",
    "s = processState(s)\n",
    "print (s.shape)\n",
    "s = env.reset()\n",
    "s = state2input(state_history, s)\n",
    "s = processState(s)\n",
    "print (s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        #We use slim.conv2d to set up our network \n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168*4],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3*4])\n",
    "#         self.conv1 = slim.conv2d( \\\n",
    "#             inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "#         self.conv2 = slim.conv2d( \\\n",
    "#             inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "#         self.conv3 = slim.conv2d( \\\n",
    "#             inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "#         self.conv4 = slim.conv2d( \\\n",
    "#             inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv1 = slim.conv2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = slim.conv2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        \n",
    "        ################################################################################\n",
    "        # TODO: Implement Dueling DQN                                                  #\n",
    "        # We take the output from the final convolutional layer i.e. self.conv4 and    #\n",
    "        # split it into separate advantage and value streams.                          #\n",
    "        # Outout: self.Advantage, self.Value                                           #\n",
    "        # Hint: Refer to Fig.1 in [Dueling DQN](https://arxiv.org/pdf/1511.06581.pdf)  #\n",
    "        #       In implementation, use tf.split to split into two branches. You may    #\n",
    "        #       use xavier_initializer for initializing the two additional linear      #\n",
    "        #       layers.                                                                # \n",
    "        ################################################################################\n",
    "        pass\n",
    "        init = tf.contrib.layers.xavier_initializer()\n",
    "        V_split, A_split = tf.split(self.conv4, 2, 3)\n",
    "#         V_split = tf.reshape(V_split, [-1, h_size])\n",
    "#         A_split = tf.reshape(A_split, [-1, h_size])\n",
    "        V_split = slim.flatten(V_split)\n",
    "        A_split = slim.flatten(A_split)\n",
    "        self.Value = tf.layers.dense(V_split, 1, kernel_initializer=init)\n",
    "        self.Advantage = tf.layers.dense(A_split, env.action_space.n, kernel_initializer=init)\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        \n",
    "        #Then combine them together to get our final Q-values. \n",
    "        #Please refer to Equation (9) in [Dueling DQN](https://arxiv.org/pdf/1511.06581.pdf)\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.action_space.n,dtype=tf.float32)\n",
    "        \n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Obtain the loss (self.loss) by taking the sum of squares difference          #\n",
    "        # between the target and prediction Q values.                                  #\n",
    "        ################################################################################\n",
    "        pass\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        self.loss = tf.reduce_sum(tf.square(self.Q-self.targetQ))\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting all the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "annealing_steps = 10000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 5000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
    "max_epLength = 500 #The max allowed length of our episode.\n",
    "load_model = True #Whether to load a saved model.\n",
    "path = \"./dqn_breakout\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./dqn_breakout/model-1000.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./dqn_breakout/model-1000.ckpt\n",
      "[2017-11-29 02:41:04,864] Restoring parameters from ./dqn_breakout/model-1000.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Episode 9 reward: 1.0\n",
      "Episode 19 reward: 1.3\n",
      "Episode 29 reward: 1.3\n",
      "Episode 39 reward: 1.4\n",
      "Episode 49 reward: 0.6\n",
      "Episode 59 reward: 0.8\n",
      "Episode 69 reward: 1.2\n",
      "Episode 79 reward: 3.2\n",
      "Episode 89 reward: 4.4\n",
      "Episode 99 reward: 8.8\n",
      "Episode 109 reward: 11.6\n",
      "Episode 119 reward: 10.8\n",
      "Episode 129 reward: 9.9\n",
      "Episode 139 reward: 10.2\n",
      "Episode 149 reward: 9.1\n",
      "Episode 159 reward: 11.1\n",
      "Episode 169 reward: 9.5\n",
      "Episode 179 reward: 10.7\n",
      "Episode 189 reward: 12.0\n",
      "Episode 199 reward: 14.2\n",
      "Episode 209 reward: 13.5\n",
      "Episode 219 reward: 17.0\n",
      "Episode 229 reward: 14.6\n",
      "Episode 239 reward: 14.1\n",
      "Episode 249 reward: 13.2\n",
      "Episode 259 reward: 13.0\n",
      "Episode 269 reward: 13.8\n",
      "Episode 279 reward: 14.1\n",
      "Episode 289 reward: 12.9\n",
      "Episode 299 reward: 16.9\n",
      "Episode 309 reward: 13.7\n",
      "Episode 319 reward: 17.6\n",
      "Episode 329 reward: 18.0\n",
      "Episode 339 reward: 19.3\n",
      "Episode 349 reward: 18.8\n",
      "Episode 359 reward: 17.6\n",
      "Episode 369 reward: 20.4\n",
      "Episode 379 reward: 20.5\n",
      "Episode 389 reward: 19.4\n",
      "Episode 399 reward: 18.1\n",
      "Episode 409 reward: 17.8\n",
      "Episode 419 reward: 24.3\n",
      "Episode 429 reward: 21.3\n",
      "Episode 439 reward: 22.6\n",
      "Episode 449 reward: 21.7\n",
      "Episode 459 reward: 19.9\n",
      "Episode 469 reward: 23.4\n",
      "Episode 479 reward: 22.6\n",
      "Episode 489 reward: 22.5\n",
      "Episode 499 reward: 20.6\n",
      "Episode 509 reward: 23.9\n",
      "Episode 519 reward: 23.6\n",
      "Episode 529 reward: 21.8\n",
      "Episode 539 reward: 22.2\n",
      "Episode 549 reward: 22.7\n",
      "Episode 559 reward: 22.8\n",
      "Episode 569 reward: 21.3\n",
      "Episode 579 reward: 23.5\n",
      "Episode 589 reward: 28.1\n",
      "Episode 599 reward: 20.5\n",
      "Episode 609 reward: 22.8\n",
      "Episode 619 reward: 20.7\n",
      "Episode 629 reward: 25.1\n",
      "Episode 639 reward: 30.2\n",
      "Episode 649 reward: 23.1\n",
      "Episode 659 reward: 24.6\n",
      "Episode 669 reward: 24.4\n",
      "Episode 679 reward: 28.8\n",
      "Episode 689 reward: 25.6\n",
      "Episode 699 reward: 24.5\n",
      "Episode 709 reward: 29.0\n",
      "Episode 719 reward: 27.6\n",
      "Episode 729 reward: 26.6\n",
      "Episode 739 reward: 33.8\n",
      "Episode 749 reward: 39.5\n",
      "Episode 759 reward: 21.9\n",
      "Episode 769 reward: 30.8\n",
      "Episode 779 reward: 34.3\n",
      "Episode 789 reward: 27.8\n",
      "Episode 799 reward: 32.3\n",
      "Episode 809 reward: 25.4\n",
      "Episode 819 reward: 31.7\n",
      "Episode 829 reward: 34.4\n",
      "Episode 839 reward: 18.3\n",
      "Episode 849 reward: 29.2\n",
      "Episode 859 reward: 41.9\n",
      "Episode 869 reward: 35.4\n",
      "Episode 879 reward: 39.1\n",
      "Episode 889 reward: 31.8\n",
      "Episode 899 reward: 37.3\n",
      "Episode 909 reward: 34.3\n",
      "Episode 919 reward: 35.7\n",
      "Episode 929 reward: 33.0\n",
      "Episode 939 reward: 36.1\n",
      "Episode 949 reward: 34.2\n",
      "Episode 959 reward: 38.2\n",
      "Episode 969 reward: 37.3\n",
      "Episode 979 reward: 33.8\n",
      "Episode 989 reward: 38.9\n",
      "Episode 999 reward: 37.0\n",
      "Saved Model\n",
      "Episode 1009 reward: 33.7\n",
      "Episode 1019 reward: 33.4\n",
      "Episode 1029 reward: 34.4\n",
      "Episode 1039 reward: 42.4\n",
      "Episode 1049 reward: 39.6\n",
      "Episode 1059 reward: 35.1\n",
      "Episode 1069 reward: 39.4\n",
      "Episode 1079 reward: 31.7\n",
      "Episode 1089 reward: 32.8\n",
      "Episode 1099 reward: 36.3\n",
      "Episode 1109 reward: 40.4\n",
      "Episode 1119 reward: 46.7\n",
      "Episode 1129 reward: 36.3\n",
      "Episode 1139 reward: 30.3\n",
      "Episode 1149 reward: 33.4\n",
      "Episode 1159 reward: 45.9\n",
      "Episode 1169 reward: 46.6\n",
      "Episode 1179 reward: 48.0\n",
      "Episode 1189 reward: 45.3\n",
      "Episode 1199 reward: 35.1\n",
      "Episode 1209 reward: 35.3\n",
      "Episode 1219 reward: 42.8\n",
      "Episode 1229 reward: 41.9\n",
      "Episode 1239 reward: 33.3\n",
      "Episode 1249 reward: 41.2\n",
      "Episode 1259 reward: 32.1\n",
      "Episode 1269 reward: 31.4\n",
      "Episode 1279 reward: 40.7\n",
      "Episode 1289 reward: 35.1\n",
      "Episode 1299 reward: 40.8\n",
      "Episode 1309 reward: 37.4\n",
      "Episode 1319 reward: 27.3\n",
      "Episode 1329 reward: 44.7\n",
      "Episode 1339 reward: 50.3\n",
      "Episode 1349 reward: 40.1\n",
      "Episode 1359 reward: 39.8\n",
      "Episode 1369 reward: 42.5\n",
      "Episode 1379 reward: 30.3\n",
      "Episode 1389 reward: 39.2\n",
      "Episode 1399 reward: 36.7\n",
      "Episode 1409 reward: 38.1\n",
      "Episode 1419 reward: 46.7\n",
      "Episode 1429 reward: 46.4\n",
      "Episode 1439 reward: 49.1\n",
      "Episode 1449 reward: 42.8\n",
      "Episode 1459 reward: 35.3\n",
      "Episode 1469 reward: 39.3\n",
      "Episode 1479 reward: 41.0\n",
      "Episode 1489 reward: 37.9\n",
      "Episode 1499 reward: 41.9\n",
      "Episode 1509 reward: 30.7\n",
      "Episode 1519 reward: 42.7\n",
      "Episode 1529 reward: 36.0\n",
      "Episode 1539 reward: 37.2\n",
      "Episode 1549 reward: 36.2\n",
      "Episode 1559 reward: 37.3\n",
      "Episode 1569 reward: 47.5\n",
      "Episode 1579 reward: 30.1\n",
      "Episode 1589 reward: 34.0\n",
      "Episode 1599 reward: 43.3\n",
      "Episode 1609 reward: 43.3\n",
      "Episode 1619 reward: 34.9\n",
      "Episode 1629 reward: 39.1\n",
      "Episode 1639 reward: 35.8\n",
      "Episode 1649 reward: 41.3\n",
      "Episode 1659 reward: 38.4\n",
      "Episode 1669 reward: 37.0\n",
      "Episode 1679 reward: 37.1\n",
      "Episode 1689 reward: 27.3\n",
      "Episode 1699 reward: 32.6\n",
      "Episode 1709 reward: 36.3\n",
      "Episode 1719 reward: 34.1\n",
      "Episode 1729 reward: 35.0\n",
      "Episode 1739 reward: 37.9\n",
      "Episode 1749 reward: 35.3\n",
      "Episode 1759 reward: 36.7\n",
      "Episode 1769 reward: 32.1\n",
      "Episode 1779 reward: 40.8\n",
      "Episode 1789 reward: 37.4\n",
      "Episode 1799 reward: 41.8\n",
      "Episode 1809 reward: 41.4\n",
      "Episode 1819 reward: 34.4\n",
      "Episode 1829 reward: 39.7\n",
      "Episode 1839 reward: 30.7\n",
      "Episode 1849 reward: 36.5\n",
      "Episode 1859 reward: 31.6\n",
      "Episode 1869 reward: 28.7\n",
      "Episode 1879 reward: 35.2\n",
      "Episode 1889 reward: 40.7\n",
      "Episode 1899 reward: 34.1\n",
      "Episode 1909 reward: 40.0\n",
      "Episode 1919 reward: 38.4\n",
      "Episode 1929 reward: 29.8\n",
      "Episode 1939 reward: 40.2\n",
      "Episode 1949 reward: 29.4\n",
      "Episode 1959 reward: 43.1\n",
      "Episode 1969 reward: 38.8\n",
      "Episode 1979 reward: 32.1\n",
      "Episode 1989 reward: 41.1\n",
      "Episode 1999 reward: 42.1\n",
      "Saved Model\n",
      "Episode 2009 reward: 35.0\n",
      "Episode 2019 reward: 38.3\n",
      "Episode 2029 reward: 41.0\n",
      "Episode 2039 reward: 35.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-ca56d484e4c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    111\u001b[0m                     \u001b[0mtarget_Q_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr_batch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtargetQN_Q_value\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mend_mat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                     \u001b[0mtarget_Q_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr_batch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtargetQN_Q_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdateModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalarInput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ms_batch\u001b[0m\u001b[1;33m,\u001b[0m                                                           \u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtarget_Q_value\u001b[0m\u001b[1;33m,\u001b[0m                                                           \u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ma_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m                     \u001b[1;31m################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[1;31m#                                 END OF YOUR CODE                             #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Jerry/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Jerry/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Jerry/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Jerry/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Jerry/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        state_history = []\n",
    "        for idx in range(4):\n",
    "            state_history.append(preprocess(s))\n",
    "        s = state2input(state_history, s)\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "#         while j < max_epLength: #If the agent takes longer than 50 moves to reach either of the blocks, end the trial.\n",
    "        while True:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            total_steps += 1\n",
    "            \n",
    "            ################################################################################\n",
    "            # TODO: Save the experience to our episode buffer.                             #\n",
    "            # You will need to do the following:                                           #\n",
    "            # (1) Get new state s1 (resized), reward r and done d from a                   #\n",
    "            # (2) Add experience to episode buffer. Hint: experience includes              #\n",
    "            #     s, a, r, s1 and d.                                                       #\n",
    "            ################################################################################\n",
    "            pass\n",
    "#             r = 0\n",
    "#             d = False\n",
    "#             for idx in range(4):\n",
    "#                 _s1, _r, _d, _ = env.step(a)\n",
    "#                 s1 = state2input(state_history, _s1)\n",
    "#                 r += _r\n",
    "#                 d = d or _d\n",
    "            s1, _r, d, _ = env.step(a)\n",
    "            s1 = state2input(state_history, s1)\n",
    "            r = transform_reward(_r)\n",
    "            s1 = processState(s1)\n",
    "            episodeBuffer.add(np.reshape(np.array([s, a, r, s1, d]), [1, 5]))\n",
    "            ################################################################################\n",
    "            #                                 END OF YOUR CODE                             #\n",
    "            ##############################################################################\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    # TODO: Implement Double-DQN                                                   #\n",
    "                    # (1) Get a random batch of experiences via experience_buffer class            #\n",
    "                    #                                                                              #\n",
    "                    # (2) Perform the Double-DQN update to the target Q-values                     #\n",
    "                    #     Hint: Use mainQN and targetQN separately to chose an action and predict  #\n",
    "                    #     the Q-values for that action.                                            #\n",
    "                    #     Then compute targetQ based on Double-DQN equation                        #\n",
    "                    #                                                                              #\n",
    "                    # (3) Update the primary network with our target values                        #\n",
    "                    ################################################################################ \n",
    "                    pass\n",
    "                    exp_batch = myBuffer.sample(batch_size)\n",
    "                    s_batch = np.vstack(exp_batch[:, 0])\n",
    "                    a_batch = exp_batch[:, 1]\n",
    "                    r_batch = exp_batch[:, 2]\n",
    "                    s1_batch = np.vstack(exp_batch[:, 3])\n",
    "                    d_batch = exp_batch[:, 4]\n",
    "#                     print (exp_batch.shape, s_batch.shape, a_batch.shape, r_batch.shape, s1_batch.shape, d_batch.shape)\n",
    "                    mainQN_predict = sess.run(mainQN.predict, feed_dict={mainQN.scalarInput:s1_batch})\n",
    "                    targetQN_Qout = sess.run(targetQN.Qout, feed_dict={targetQN.scalarInput:s1_batch})\n",
    "                    targetQN_Q_value = targetQN_Qout[range(batch_size), mainQN_predict]\n",
    "                    end_mat = -(d_batch-1)\n",
    "                    target_Q_value = r_batch + y*targetQN_Q_value*end_mat\n",
    "                    target_Q_value = r_batch + y*targetQN_Q_value\n",
    "                    sess.run(mainQN.updateModel, feed_dict={mainQN.scalarInput:s_batch,\\\n",
    "                                                           mainQN.targetQ:target_Q_value,\\\n",
    "                                                           mainQN.actions:a_batch})\n",
    "                    ################################################################################\n",
    "                    #                                 END OF YOUR CODE                             #\n",
    "                    ################################################################################\n",
    "                           \n",
    "                    updateTarget(targetOps,sess) #Update the target network toward the primary network.\n",
    "            rAll += _r\n",
    "            s = s1\n",
    "            \n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        #Periodically save the model. \n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "            print(\"Saved Model\")\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(\"Episode\",i,\"reward:\",np.mean(rList[-10:]))\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "print(\"Mean reward per episode: \" + str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes about 40 minutes to train 5000 episodes in Lab 4 machines. Mean reward per episode (50 steps) should be around 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking network learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean reward over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4951bcc630>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUz8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a39adebe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
