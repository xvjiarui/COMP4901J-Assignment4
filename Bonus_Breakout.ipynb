{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imresize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: BreakoutDeterministic-v4\n",
      "[2017-11-28 16:30:15,577] Making new env: BreakoutDeterministic-v4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# print(gym.envs.registry.all())\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "observation = env.reset() # This gets us the image\n",
    "print (observation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_close',\n",
       " '_closed',\n",
       " '_elapsed_seconds',\n",
       " '_elapsed_steps',\n",
       " '_ensure_no_double_wrap',\n",
       " '_env_closer_id',\n",
       " '_episode_started_at',\n",
       " '_max_episode_seconds',\n",
       " '_max_episode_steps',\n",
       " '_owns_render',\n",
       " '_past_limit',\n",
       " '_render',\n",
       " '_reset',\n",
       " '_seed',\n",
       " '_spec',\n",
       " '_step',\n",
       " 'action_space',\n",
       " 'class_name',\n",
       " 'close',\n",
       " 'configure',\n",
       " 'env',\n",
       " 'metadata',\n",
       " 'observation_space',\n",
       " 'render',\n",
       " 'reset',\n",
       " 'reward_range',\n",
       " 'seed',\n",
       " 'spec',\n",
       " 'step',\n",
       " 'unwrapped']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame, reward, is_done, _ = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3) uint8\n"
     ]
    }
   ],
   "source": [
    "print (frame.shape, frame.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f71c4dd0e10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nITJL+j+3QFvlMGmcOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f71c735fda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_grayscale(img):\n",
    "    return np.dot(img[...,:3], [0.299, 0.587, 0.114]).astype(np.uint8)\n",
    "\n",
    "def crop(img,cropx=None,cropy=None):\n",
    "    y,x,c = img.shape\n",
    "    if (cropx is None) :\n",
    "        cropx = min(y, x)\n",
    "    if (cropy is None) :\n",
    "        cropy = min(y, x)\n",
    "    startx = x//2 - cropx//2\n",
    "    starty = y - cropy    \n",
    "    return img[starty:starty+cropy, startx:startx+cropx, :]\n",
    "\n",
    "def preprocess(img):\n",
    "#     return imresize(to_grayscale(crop(img))/127.5-1, (84, 84), 'cubic', 'F')\n",
    "    return imresize(crop(img)/127.5-1, (84, 84), 'cubic', 'RGB')\n",
    "\n",
    "def transform_reward(reward):\n",
    "        return np.sign(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 3) uint8 255 0 46.4648053666\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfLMMOfhmGfp/zI4G5qUmq78AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f71c055a1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = preprocess(frame)\n",
    "plt.imshow(a)\n",
    "print (a.shape, a.dtype, np.max(a), np.min(a), np.mean(a))\n",
    "# plt.imshow(to_grayscale(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[21168*4]) # 84 x 84 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addToHistory(his, s):\n",
    "    his.pop(0)\n",
    "    his.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def his2np(his):\n",
    "    return np.concatenate(tuple(his), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def state2input(his, s):\n",
    "    addToHistory(his, preprocess(s))\n",
    "    return his2np(his)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "(84672,)\n",
      "(84672,)\n"
     ]
    }
   ],
   "source": [
    "state_history = []\n",
    "s = env.reset()\n",
    "for idx in range(4):\n",
    "    state_history.append(preprocess(s))\n",
    "print (s.shape)\n",
    "s = state2input(state_history, s)\n",
    "s = processState(s)\n",
    "print (s.shape)\n",
    "s = env.reset()\n",
    "s = state2input(state_history, s)\n",
    "s = processState(s)\n",
    "print (s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        #We use slim.conv2d to set up our network \n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168*4],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3*4])\n",
    "#         self.conv1 = slim.conv2d( \\\n",
    "#             inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "#         self.conv2 = slim.conv2d( \\\n",
    "#             inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "#         self.conv3 = slim.conv2d( \\\n",
    "#             inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "#         self.conv4 = slim.conv2d( \\\n",
    "#             inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv1 = slim.conv2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = slim.conv2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        \n",
    "        ################################################################################\n",
    "        # TODO: Implement Dueling DQN                                                  #\n",
    "        # We take the output from the final convolutional layer i.e. self.conv4 and    #\n",
    "        # split it into separate advantage and value streams.                          #\n",
    "        # Outout: self.Advantage, self.Value                                           #\n",
    "        # Hint: Refer to Fig.1 in [Dueling DQN](https://arxiv.org/pdf/1511.06581.pdf)  #\n",
    "        #       In implementation, use tf.split to split into two branches. You may    #\n",
    "        #       use xavier_initializer for initializing the two additional linear      #\n",
    "        #       layers.                                                                # \n",
    "        ################################################################################\n",
    "        pass\n",
    "        init = tf.contrib.layers.xavier_initializer()\n",
    "        V_split, A_split = tf.split(self.conv4, 2, 3)\n",
    "#         V_split = tf.reshape(V_split, [-1, h_size])\n",
    "#         A_split = tf.reshape(A_split, [-1, h_size])\n",
    "        V_split = slim.flatten(V_split)\n",
    "        A_split = slim.flatten(A_split)\n",
    "        self.Value = tf.layers.dense(V_split, 1, kernel_initializer=init)\n",
    "        self.Advantage = tf.layers.dense(A_split, env.action_space.n, kernel_initializer=init)\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        \n",
    "        #Then combine them together to get our final Q-values. \n",
    "        #Please refer to Equation (9) in [Dueling DQN](https://arxiv.org/pdf/1511.06581.pdf)\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.action_space.n,dtype=tf.float32)\n",
    "        \n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Obtain the loss (self.loss) by taking the sum of squares difference          #\n",
    "        # between the target and prediction Q values.                                  #\n",
    "        ################################################################################\n",
    "        pass\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        self.loss = tf.reduce_sum(tf.square(self.Q-self.targetQ))\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting all the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "annealing_steps = 10000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 5000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
    "max_epLength = 500 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./dqn_breakout\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Episode 9 reward: 1.3\n",
      "Episode 19 reward: 1.1\n",
      "Episode 29 reward: 1.4\n",
      "Episode 39 reward: 1.1\n",
      "Episode 49 reward: 0.8\n",
      "Episode 59 reward: 1.7\n",
      "Episode 69 reward: 0.9\n",
      "Episode 79 reward: 1.4\n",
      "Episode 89 reward: 1.4\n",
      "Episode 99 reward: 1.5\n",
      "Episode 109 reward: 1.0\n",
      "Episode 119 reward: 1.1\n",
      "Episode 129 reward: 1.0\n",
      "Episode 139 reward: 0.8\n",
      "Episode 149 reward: 1.2\n",
      "Episode 159 reward: 1.5\n",
      "Episode 169 reward: 0.8\n",
      "Episode 179 reward: 1.4\n",
      "Episode 189 reward: 2.0\n",
      "Episode 199 reward: 1.4\n",
      "Episode 209 reward: 1.5\n",
      "Episode 219 reward: 1.9\n",
      "Episode 229 reward: 1.7\n",
      "Episode 239 reward: 3.4\n",
      "Episode 249 reward: 2.2\n",
      "Episode 259 reward: 0.7\n",
      "Episode 269 reward: 1.0\n",
      "Episode 279 reward: 0.8\n",
      "Episode 289 reward: 1.3\n",
      "Episode 299 reward: 1.2\n",
      "Episode 309 reward: 0.4\n",
      "Episode 319 reward: 0.9\n",
      "Episode 329 reward: 0.4\n",
      "Episode 339 reward: 0.7\n",
      "Episode 349 reward: 1.7\n",
      "Episode 359 reward: 2.3\n",
      "Episode 369 reward: 2.3\n",
      "Episode 379 reward: 2.2\n",
      "Episode 389 reward: 3.6\n",
      "Episode 399 reward: 4.3\n",
      "Episode 409 reward: 3.7\n",
      "Episode 419 reward: 3.9\n",
      "Episode 429 reward: 4.3\n",
      "Episode 439 reward: 3.3\n",
      "Episode 449 reward: 4.6\n",
      "Episode 459 reward: 3.9\n",
      "Episode 469 reward: 4.0\n",
      "Episode 479 reward: 4.5\n",
      "Episode 489 reward: 4.9\n",
      "Episode 499 reward: 5.4\n",
      "Episode 509 reward: 4.9\n",
      "Episode 519 reward: 4.7\n",
      "Episode 529 reward: 4.3\n",
      "Episode 539 reward: 4.7\n",
      "Episode 549 reward: 4.3\n",
      "Episode 559 reward: 4.5\n",
      "Episode 569 reward: 4.5\n",
      "Episode 579 reward: 5.0\n",
      "Episode 589 reward: 4.6\n",
      "Episode 599 reward: 3.8\n",
      "Episode 609 reward: 4.1\n",
      "Episode 619 reward: 3.7\n",
      "Episode 629 reward: 3.3\n",
      "Episode 639 reward: 4.2\n",
      "Episode 649 reward: 6.0\n",
      "Episode 659 reward: 6.4\n",
      "Episode 669 reward: 6.4\n",
      "Episode 679 reward: 6.6\n",
      "Episode 689 reward: 6.3\n",
      "Episode 699 reward: 6.5\n",
      "Episode 709 reward: 7.2\n",
      "Episode 719 reward: 6.4\n",
      "Episode 729 reward: 6.8\n",
      "Episode 739 reward: 7.4\n",
      "Episode 749 reward: 7.2\n",
      "Episode 759 reward: 7.8\n",
      "Episode 769 reward: 7.3\n",
      "Episode 779 reward: 6.8\n",
      "Episode 789 reward: 7.4\n",
      "Episode 799 reward: 8.6\n",
      "Episode 809 reward: 8.0\n",
      "Episode 819 reward: 7.2\n",
      "Episode 829 reward: 6.2\n",
      "Episode 839 reward: 6.2\n",
      "Episode 849 reward: 7.7\n",
      "Episode 859 reward: 6.5\n",
      "Episode 869 reward: 6.4\n",
      "Episode 879 reward: 7.8\n",
      "Episode 889 reward: 7.4\n",
      "Episode 899 reward: 7.2\n",
      "Episode 909 reward: 5.5\n",
      "Episode 919 reward: 5.6\n",
      "Episode 929 reward: 5.6\n",
      "Episode 939 reward: 8.8\n",
      "Episode 949 reward: 8.5\n",
      "Episode 959 reward: 11.0\n",
      "Episode 969 reward: 10.1\n",
      "Episode 979 reward: 8.1\n",
      "Episode 989 reward: 8.1\n",
      "Episode 999 reward: 10.0\n",
      "Saved Model\n",
      "Episode 1009 reward: 8.6\n",
      "Episode 1019 reward: 9.8\n",
      "Episode 1029 reward: 7.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-60cc0218671f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;31m#                     target_Q_value = r_batch + y*targetQN_Q_value*end_mat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                     \u001b[0mtarget_Q_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr_batch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtargetQN_Q_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdateModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalarInput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ms_batch\u001b[0m\u001b[1;33m,\u001b[0m                                                           \u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtarget_Q_value\u001b[0m\u001b[1;33m,\u001b[0m                                                           \u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ma_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m                     \u001b[1;31m################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                     \u001b[1;31m#                                 END OF YOUR CODE                             #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Jerry/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Jerry/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Jerry/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Jerry/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Jerry/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        state_history = []\n",
    "        for idx in range(4):\n",
    "            state_history.append(preprocess(s))\n",
    "        s = state2input(state_history, s)\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "#         while j < max_epLength: #If the agent takes longer than 50 moves to reach either of the blocks, end the trial.\n",
    "        while True:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            total_steps += 1\n",
    "            \n",
    "            ################################################################################\n",
    "            # TODO: Save the experience to our episode buffer.                             #\n",
    "            # You will need to do the following:                                           #\n",
    "            # (1) Get new state s1 (resized), reward r and done d from a                   #\n",
    "            # (2) Add experience to episode buffer. Hint: experience includes              #\n",
    "            #     s, a, r, s1 and d.                                                       #\n",
    "            ################################################################################\n",
    "            pass\n",
    "#             r = 0\n",
    "#             d = False\n",
    "#             for idx in range(4):\n",
    "#                 _s1, _r, _d, _ = env.step(a)\n",
    "#                 s1 = state2input(state_history, _s1)\n",
    "#                 r += _r\n",
    "#                 d = d or _d\n",
    "            s1, r, d, _ = env.step(a)\n",
    "                \n",
    "            r = transform_reward(r)\n",
    "            s1 = processState(s1)\n",
    "            episodeBuffer.add(np.reshape(np.array([s, a, r, s1, d]), [1, 5]))\n",
    "            ################################################################################\n",
    "            #                                 END OF YOUR CODE                             #\n",
    "            ##############################################################################\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    # TODO: Implement Double-DQN                                                   #\n",
    "                    # (1) Get a random batch of experiences via experience_buffer class            #\n",
    "                    #                                                                              #\n",
    "                    # (2) Perform the Double-DQN update to the target Q-values                     #\n",
    "                    #     Hint: Use mainQN and targetQN separately to chose an action and predict  #\n",
    "                    #     the Q-values for that action.                                            #\n",
    "                    #     Then compute targetQ based on Double-DQN equation                        #\n",
    "                    #                                                                              #\n",
    "                    # (3) Update the primary network with our target values                        #\n",
    "                    ################################################################################ \n",
    "                    pass\n",
    "                    exp_batch = myBuffer.sample(batch_size)\n",
    "                    s_batch = np.vstack(exp_batch[:, 0])\n",
    "                    a_batch = exp_batch[:, 1]\n",
    "                    r_batch = exp_batch[:, 2]\n",
    "                    s1_batch = np.vstack(exp_batch[:, 3])\n",
    "                    d_batch = exp_batch[:, 4]\n",
    "#                     print (exp_batch.shape, s_batch.shape, a_batch.shape, r_batch.shape, s1_batch.shape, d_batch.shape)\n",
    "                    mainQN_predict = sess.run(mainQN.predict, feed_dict={mainQN.scalarInput:s1_batch})\n",
    "                    targetQN_Qout = sess.run(targetQN.Qout, feed_dict={targetQN.scalarInput:s1_batch})\n",
    "                    targetQN_Q_value = targetQN_Qout[range(batch_size), mainQN_predict]\n",
    "                    end_mat = -(d_batch-1)\n",
    "                    target_Q_value = r_batch + y*targetQN_Q_value*end_mat\n",
    "                    target_Q_value = r_batch + y*targetQN_Q_value\n",
    "                    sess.run(mainQN.updateModel, feed_dict={mainQN.scalarInput:s_batch,\\\n",
    "                                                           mainQN.targetQ:target_Q_value,\\\n",
    "                                                           mainQN.actions:a_batch})\n",
    "                    ################################################################################\n",
    "                    #                                 END OF YOUR CODE                             #\n",
    "                    ################################################################################\n",
    "                           \n",
    "                    updateTarget(targetOps,sess) #Update the target network toward the primary network.\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        #Periodically save the model. \n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "            print(\"Saved Model\")\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(\"Episode\",i,\"reward:\",np.mean(rList[-10:]))\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "print(\"Mean reward per episode: \" + str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes about 40 minutes to train 5000 episodes in Lab 4 machines. Mean reward per episode (50 steps) should be around 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking network learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean reward over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
